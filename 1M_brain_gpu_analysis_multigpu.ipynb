{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPIDS & Scanpy Single-Cell RNA-seq Workflow on 1.3 Million Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020, NVIDIA CORPORATION.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\") you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0 \n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a single-cell RNA analysis workflow that begins with preprocessing a count matrix of size `(n_gene, n_cell)` and results in a visualization of the clustered cells for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we use a dataset of 1M brain cells with Unified Virtual Memory to oversubscribe GPU memory. See the README for instructions to download this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to the notebooks directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change into the *notebooks* directory. You may need to modify your path depending on where you cloned the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "\n",
    "import dask\n",
    "import time\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy as cp\n",
    "\n",
    "import os, wget\n",
    "\n",
    "from cuml.decomposition import PCA\n",
    "from cuml.manifold import TSNE\n",
    "from cuml.cluster import KMeans\n",
    "\n",
    "from dask_cuda import initialize, LocalCUDACluster\n",
    "from dask.distributed import Client, default_client\n",
    "\n",
    "import rapids_scanpy_funcs\n",
    "import utils\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#cuml.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the RAPIDS memory manager to enable Unified Virtual Memory management, which allows us to oversubscribe the GPU memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rmm\n",
    "from rmm.allocators.cupy import rmm_cupy_allocator\n",
    "\n",
    "def set_mem():\n",
    "    rmm.reinitialize(managed_memory=True)\n",
    "    cp.cuda.set_allocator(rmm_cupy_allocator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `preprocessing_gpus` below to specify the GPUs to use for preprocessing. For example, numbers 0-7 can be used on a machine with 8 gpus. Specifying a specific number, such as 5, will use only the 6th GPU on the machine. In practice, it's often a good idea to use GPUs 1-7 for pre-processing and GPU0 for downstream clustering, visualization, and differential gene expression steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_gpus=\"1, 2, 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=preprocessing_gpus)\n",
    "client = Client(cluster)    \n",
    "\n",
    "set_mem()\n",
    "client.run(set_mem)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we provide the path to the sparse `.h5ad` file containing the count matrix to analyze.\n",
    "\n",
    "To run this notebook using your own dataset, please see the README for instructions to convert your own count matrix into this format. Then, replace the path in the cell below with the path to your generated `.h5ad` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/data/1M_brain_cells_10X.sparse.h5ad\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print('Downloading import file...')\n",
    "    os.makedirs('/data', exist_ok=True)\n",
    "    wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',\n",
    "              input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/data/1M_brain_cells_10X.sparse.h5ad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marker genes\n",
    "MITO_GENE_PREFIX = \"mt-\" # Prefix for mitochondrial genes to regress out\n",
    "markers = [\"Stmn2\", \"Hes1\", \"Olig1\"] # Marker genes for visualization\n",
    "\n",
    "# filtering cells\n",
    "min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed \n",
    "max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed \n",
    "\n",
    "# filtering genes\n",
    "n_top_genes = 4000 # Number of highly variable genes to retain\n",
    "\n",
    "# PCA\n",
    "n_components = 50 # Number of principal components to compute\n",
    "\n",
    "# t-SNE\n",
    "tsne_n_pcs = 20 # Number of principal components to use for t-SNE\n",
    "\n",
    "# k-means\n",
    "k = 35 # Number of clusters for k-means\n",
    "\n",
    "# KNN\n",
    "n_neighbors = 15 # Number of nearest neighbors for KNN graph\n",
    "knn_n_pcs = 50 # Number of principal components to use for finding nearest neighbors\n",
    "\n",
    "# UMAP\n",
    "umap_min_dist = 0.3 \n",
    "umap_spread = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = len(client.scheduler_info()['workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load_preprocess_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we load the sparse count matrix from the `.h5ad` file into GPU using a custom function. While reading the dataset, filters are applied on the count matrix to remove cells with an extreme number of genes expressed. Genes will zero expression in all cells are also eliminated. \n",
    "\n",
    "The custom function uses [Dask](https://dask.org) to partition data. The above mentioned filters are applied on individual partitions. Usage of Dask along with cupy provides the following benefits:\n",
    "- Parallelized data loading when multiple GPUs are available\n",
    "- Ability to partition the data allows pre-processing large datasets\n",
    "\n",
    "Filters are applied on individual batches of cells. Elementwise or cell-level normalization operations are also performed while reading. For this example, the following two operations are performed:\n",
    "- Normalize the count matrix so that the total counts in each cell sum to 1e4.\n",
    "- Log transform the count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import rapids_scanpy_funcs\n",
    "\n",
    "def partial_post_processor(partial_data):\n",
    "    partial_data = rapids_scanpy_funcs.normalize_total(partial_data, target_sum=1e4)\n",
    "    return partial_data.log1p()\n",
    "\n",
    "dask_sparse_arr, genes, query = rapids_scanpy_funcs.read_with_filter(client,\n",
    "                                                       input_file,\n",
    "                                                       min_genes_per_cell=min_genes_per_cell,\n",
    "                                                       max_genes_per_cell=max_genes_per_cell,\n",
    "                                                       partial_post_processor=partial_post_processor)\n",
    "dask_sparse_arr = dask_sparse_arr.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the shape of the resulting sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_sparse_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Most Variable Genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before filtering the count matrix, we save the 'raw' expression values of the marker genes to use for labeling cells afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "marker_genes_raw = {}\n",
    "i = 0\n",
    "for index in genes[genes.isin(markers)].index.to_arrow().to_pylist():\n",
    "    marker_genes_raw[markers[i]] = dask_sparse_arr[:, index].compute().toarray().ravel()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the count matrix to retain only the most variable genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hvg = rapids_scanpy_funcs.highly_variable_genes_filter(client, dask_sparse_arr, genes, n_top_genes=n_top_genes)\n",
    "\n",
    "genes = genes[hvg]\n",
    "dask_sparse_arr = dask_sparse_arr[:, hvg]\n",
    "sparse_gpu_array = dask_sparse_arr.compute()\n",
    "\n",
    "del hvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regress out confounding factors (number of counts, mitochondrial gene expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform regression on the count matrix to correct for confounding factors -  for example purposes, we use the number of counts and the expression of mitochondrial genes (named starting with `mt-`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the total counts and the percentage of mitochondrial counts for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sparse_gpu_array = sparse_gpu_array.tocsc()\n",
    "\n",
    "mito_genes = genes.str.startswith(MITO_GENE_PREFIX).values\n",
    "n_counts = sparse_gpu_array.sum(axis=1)\n",
    "percent_mito = (sparse_gpu_array[:,mito_genes].sum(axis=1) / n_counts).ravel()\n",
    "\n",
    "n_counts = cp.array(n_counts).ravel()\n",
    "percent_mito = cp.array(percent_mito).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sparse_gpu_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_rows = dask_sparse_arr.shape[0]\n",
    "n_cols = dask_sparse_arr.shape[1]\n",
    "cols_per_worker = int(n_cols / n_workers)\n",
    "dask_sparse_arr = dask_sparse_arr.map_blocks(lambda x: x.todense(), dtype=\"float32\", meta=cp.array(cp.zeros((0,)))).T\n",
    "dask_sparse_arr = dask_sparse_arr.rechunk((cols_per_worker, n_rows)).persist()\n",
    "dask_sparse_arr.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import math\n",
    "dask_sparse_arr = dask_sparse_arr.map_blocks(lambda x: rapids_scanpy_funcs.regress_out(x.T, n_counts, percent_mito).T, dtype=\"float32\", meta=cp.array(cp.zeros(0,))).T\n",
    "dask_sparse_arr = dask_sparse_arr.rechunk((math.ceil(n_rows/n_workers), n_cols)).persist()\n",
    "dask_sparse_arr.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we scale the count matrix to obtain a z-score and apply a cutoff value of 10 standard deviations, obtaining the preprocessed count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean = dask_sparse_arr.mean(axis=0)\n",
    "dask_sparse_arr -= mean\n",
    "stddev = cp.sqrt(dask_sparse_arr.var(axis=0).compute())\n",
    "dask_sparse_arr /= stddev\n",
    "dask_sparse_arr = dask.array.clip(dask_sparse_arr, -10, 10).persist()\n",
    "del mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load_preprocess_time = time.time()\n",
    "print(\"Total data load and preprocessing time: %s\" % (data_load_preprocess_time-data_load_preprocess_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster & Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PCA to reduce the dimensionality of the matrix to its top 50 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cuml.dask.decomposition import PCA\n",
    "pca_data = PCA(n_components=50).fit_transform(dask_sparse_arr)\n",
    "pca_data.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the preprocessed count matrix as an AnnData object, which is currently in host memory. We also add the expression levels of the marker genes as observations to the annData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_pca = pca_data.compute()\n",
    "X = dask_sparse_arr.compute().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adata = anndata.AnnData(X=X)\n",
    "adata.var_names = genes.to_numpy()\n",
    "adata.obsm[\"X_pca\"] = local_pca.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pca_data\n",
    "del dask_sparse_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE + K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cluster the cells using k-means on the principal components. For example purposes, we set k=35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adata.obsm['X_tsne'] = TSNE().fit_transform(adata.obsm[\"X_pca\"][:,:tsne_n_pcs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cuml.cluster import KMeans\n",
    "kmeans_labels = KMeans(n_clusters=k, init=\"k-means||\", random_state=0).fit_predict(local_pca)\n",
    "adata.obs['kmeans'] = kmeans_labels.get().astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the cells using t-SNE and label cells by color according to the k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pl.tsne(adata, color=[\"kmeans\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP + Graph clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the cells using the UMAP algorithm in Rapids. Before UMAP, we need to construct a k-nearest neighbors graph in which each cell is connected to its nearest neighbors. This can be done conveniently using rapids functionality already integrated into Scanpy.\n",
    "\n",
    "Note that Scanpy uses an approximation to the nearest neighbors on the CPU while the GPU version performs an exact search. While both methods are known to yield useful results, some differences in the resulting visualization and clusters can be observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UMAP function from Rapids is also integrated into Scanpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pp.neighbors(adata, n_neighbors=n_neighbors, n_pcs=knn_n_pcs, method='rapids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.tl.umap(adata, min_dist=umap_min_dist, spread=umap_spread, method='rapids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the Louvain algorithm for graph-based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.tl.louvain(adata, flavor='rapids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the cells using the UMAP visualization, and using the Louvain clusters as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pl.umap(adata, color=[\"louvain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the Leiden clustering method in RAPIDS. This method has not been integrated into Scanpy and needs to be called separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adata.obs['leiden'] = rapids_scanpy_funcs.leiden(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pl.umap(adata, color=[\"leiden\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Differential expression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done clustering, we can compute a ranking for the highly differential genes in each cluster. Here we use the Louvain clusters as labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use logistic regression to identify the top 50 genes distinguishing each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores, names, reference = rapids_scanpy_funcs.rank_genes_groups(\n",
    "    adata, \n",
    "    groupby=\"louvain\",  \n",
    "    n_genes=50, groups='all', reference='rest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "adata.uns[\"rank_genes_groups\"] = {}\n",
    "adata.uns[\"rank_genes_groups\"][\"params\"] = dict(groupby=\"louvain\", method=\"logreg\", reference=reference, use_raw=False)\n",
    "adata.uns[\"rank_genes_groups\"]['scores'] = scores\n",
    "adata.uns[\"rank_genes_groups\"]['names'] = names\n",
    "sc.pl.rank_genes_groups(adata, n_genes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full time: %s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
